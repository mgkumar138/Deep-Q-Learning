import gym
import numpy as np
from keras.layers import Dense, Input
from keras.models import Model
from keras.callbacks import TensorBoard, History
import matplotlib.pyplot as plt
from matplotlib import style

### Plots ###
style.use('fivethirtyeight')
fig = plt.figure()
ax1 = fig.add_subplot(1,2,1)
ax2 = fig.add_subplot(1,2,2)

### Environment ###
env = gym.make('NChain-v0')
num_a = env.action_space.n
if env.observation_space.shape == ():
    num_s = env.observation_space.n
else:
    num_s = env.observation_space.shape[0]

### Model ###

data_input = Input(shape=(num_s,), name='data_input') # Keras functional API codes

h1 = Dense(10, activation='relu')(data_input)
prediction_output = Dense(2, activation='linear', name='prediction_output')(h1)

model = Model(inputs=data_input, outputs=prediction_output)
model.compile(optimizer='adam',
              loss='mse', # loss function is mean square error between target and current Q value
              metrics=['mae'])
# tensorboard = TensorBoard(log_dir="logs/{}".format(time())) in model.fit(callbacks=[tensorboard])

### DQN, no q table produced. instead each reward given state&action predicted ###

num_iteration = 100
num_episodes = 100
iterations = []
reward_ite = []

for k in range(num_iteration):
    y = 0.95
    lr = 0.9
    eps = 0.5
    decay_factor = 0.99
    r_avg_list = []
    episodes = []
    reward_epi = []
    loss = []
    loss_log = []

    for i in range(num_episodes):
        s = env.reset()
        eps *= decay_factor
        done = False
        r_sum = 0
        hist = 0
        done_count = 0
        while not done:
            done_count +=1
            #env.render()
            if np.random.random() < eps:
                a = np.random.randint(0, num_a) # explore more with increasing episodes
            else:
                # np.identity(num_s)[s:s + 1]) converts states to one hot coded vector given num_s bits
                # spit out both reward for current state
                target_vec = model.predict(np.identity(num_s)[s:s + 1])[0] # [0] to choose 1st object
                # select action with highest r given state
                a = np.argmax(target_vec)

             # find reward for action at state and find new state
            new_s, r, done, info = env.step(a)
            # sum total reward gained from experienced state-action reward
            r_sum += r

            # given new state, get NN to spit out reward for both actions and choose the max reward
            new_s_rewards = model.predict(np.identity(num_s)[new_s:new_s + 1])
            target = r + y*np.max(new_s_rewards)

            # update the reward for the chosen action given current state,
            # not new state, to modify training for next iteration
            target_vec = model.predict(np.identity(num_s)[s:s + 1])[0]  # [0] to choose 1st object
            target_vec[a] = target

            # train NN only 1 epoch with updated reward for action on current state
            # where loss function is the MSE between target and output
            history = model.fit(np.identity(num_s)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)
            # update current state with new state for next cycle of training
            s = new_s
            hist += history.history["loss"][0]

        r_avg_list.append(r_sum/1000) # find reward per game, normalise to 1000 while loop
        loss_log.append(hist/1000)
        print("Avg Reward = {} for Episode {} of Iteration {}".format(r_avg_list[-1], i + 1, k + 1))

        episodes.append(i+1)
        reward_epi.append(r_avg_list[-1])
        loss.append(loss_log[-1])

        ax1.plot(episodes,reward_epi)
        ax1.set_title('Average Rewards every episode')
        ax1.set_xlabel('Episodes')
        ax1.set_ylabel('Reward')
        plt.pause(0.001)

    iterations.append(k+1)
    reward_ite.append(r_avg_list[-1])

    ax2.plot(iterations,reward_ite)
    ax2.set_title('Average Rewards in game iteration')
    ax2.set_xlabel('Iteration')
    ax2.set_ylabel('Reward')
    plt.pause(0.001)

plt.savefig('KerasRLit'+str(num_iteration)+'ep'+str(num_episodes)+'.png')
plt.show()
